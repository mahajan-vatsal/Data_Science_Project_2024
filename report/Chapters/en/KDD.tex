%%%%%%%%%%%%%%%%%%%%%%%%
%
% $Autor: Informatics Students $
% $Datum: 2020-07-24 09:05:07Z $
% $Pfad: GDV/Vortraege/latex - Ausarbeitung/Kapitel/Einleitung.tex $
% $Version: 4732 $
%
%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Knowledge Discovery in Databases (KDD) Process}

KDD plays a pivotal role in helping organizations navigate the over whelming volumes of data generated in today’s information-driven world. KDD is a structural approach to data analysis to discover and make explicit knowledge available in extensive data sets \cite{Wings:2024}. This methodology involves a series of well-defined steps, including data selection, preprocessing, transformation and analysis, leading to the generation of actionable knowledge as shown in the figure ~\ref{KDD Process}. \cite{KDD:2000}

\begin{figure}
	\begin{center}
		\includegraphics[width=0.7\linewidth]{Images/KDD.png}
		\caption{KDD Process}
		\label{KDD Process} 
		\cite{Wings:2024}
	\end{center}
\end{figure}

KDD is particularly relevant for analyzing log file data in the context of threat and weakness analysis. The structured approach ensures that patterns and anomalies are identified efficiently, enabling the identification of targeted solutions for improving system security and operational performance. \cite{IEEE:1997}

In this project, the KDD process will serve as the backbone for organizing and implementing analytics tasks, ensuring a systematic exploration of the data to uncover valuable insights.

\section{Topic Description}
This section of the report outlines the objective to focuses on identifying anomalies, vulnerabilities, and inefficiencies within the dataset. The primary goal is to uncover potential security risks, operational weaknesses, and performance bottlenecks through a comprehensive analysis of system logs.
The Knowledge Discovery in Databases (KDD) process was utilized to manage the complexity of the dataset effectively. This structured methodology enabled efficient data handling, cleaning and preparation. By leveraging the KDD process, the analysis distinguished patterns indicative of normal and abnormal behavior, identified clusters of high-risk events based on suspicious trace of useragents and paths.
By combining statistical techniques, machine learning algorithms, and domain expertise, the analysis contributed significantly to the overall enhancement of system monitoring and management.

\subsection{Special Challenges}

\begin{itemize}
	\item The challenges in this dataset includes efficiently handling the high volume of log entries.
	\item  Additionally, the dataset used in this project was not labeled, which posed difficulties for supervised analysis.
	\item Another challenge was that log entries were not sequentially arranged by \SHELL{TraceId}, as logs are ordered by timestamp due to concurrent requests. Grouping and organizing requests using \SHELL{TraceId} was necessary for accurate analysis, adding complexity to preprocessing.
	
\end{itemize}

\section{Database}

The database used in this project comprised of log files collected from the system. The analysis was performed collectively on nine log files dated 20th November, 21st November, 23rd November, 24th November, 26th November, 27th November, 29th November, 30th November, and 1st December, 2024 as shown in the figure ~\ref{RawInput}.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.9\linewidth]{Images/RawInput.png}
		\caption{Database - Log file}
		\label{RawInput} 
	\end{center}
\end{figure}

\section{Data Selection}

Data selection is a initial phase of the KDD process that directly influences the project’s outcomes. It involves examining the input log files to identify relevant portions for analysis,  ensuring the focus remains on meaningful and actionable insights.
% It focuses on identifying relevant portions of the log files while filtering out irrelevant data for analysis, ensuring that the analysis is focused on meaningful and actionable insights.

\subsection{Origin}
Logs generated by monitoring tools and event management systems, specifically collected from the Graylog server.

\subsection{Data Format}
The file format is \FILE{.log}.

\subsection{Features}
The dataset includes log entries with the following attributes
	\begin{itemize}
	\item Timestamps
	\item PID
	\item Logger
	\item Message
	\item Scope (e.g., TraceId, RequestID)
	\item Application
	\item State
	\item EventID
\end{itemize}

\subsection{Size}
The dataset is approximately 3.34 GB size.

\section{Data Preparation}
Data preparation in the Knowledge Discovery in Databases (KDD) process, ensures the dataset is clean, consistent and ready for analysis. 

\subsection{Merging and Conversion}
\begin{itemize}
	\item The initial step involved merging all nine individual log files into a single consolidated file. 
	\item Then the merged file, originally in format \FILE{.log}, were converted to CSV file to facilitate easier manipulation as shown in the figure ~\ref{csvFile}.
\end{itemize}

\begin{figure}
	\begin{center}
		\includegraphics[width=0.9\linewidth]{Images/csvFile.png}
		\caption{Converted CSV file}
		\label{csvFile} 
	\end{center}
\end{figure}

\subsection{Initial Cleaning}
\begin{itemize} 
	\item Excluding entries without a valid TraceId that contain generic error or warning messages as shown in figure ~\ref{error}.
\end{itemize}
\begin{figure}
	\begin{center}
		\includegraphics[width=0.9\linewidth]{Images/error.png}
		\caption{Error log}
		\label{error} 
	\end{center}
\end{figure}

\subsection{Key Attributes}
The following were the key parameters that were identified and prioritized for analysis,
\begin{itemize}
	\item \SHELL{TraceId}: Used to track individual requests across log entries, enabling the grouping and analysis of entire request flows.
	\item \SHELL{HTTP Status Code}: Identifies the result of a request (e.g., success, client errors, or server errors). Codes such as 200, 404 and 500 provide critical insights into system health.
	\item \SHELL{Path}: Represents the API endpoint or resource accessed during a request, useful for pinpointing affected components or services.
	\item \SHELL{User-Agent}: Provides details about the client or system making the request, helping to identify trends in usage or unusual activity from specific sources.
\end{itemize}

\section{Data Transformation}

This section focuses on manipulating and reshaping the cleaned data into a structured format ready for analysis. The key transformation includes,

\begin{enumerate}
	\item Grouping log entries by \SHELL{TraceId} to reconstruct complete request flows.
	\item Parsing relevant fields (\SHELL{TraceId, HTTP Status Code, Path, User-Agent}) from nested JSON structures.
	\item Transforming each row to represent a unique log entry with all associated fields (e.g., aligning \SHELL{TraceId} with its corresponding attributes) as shown in figure ~\ref{DataTrans}.
\end{enumerate}

\begin{figure}
	\begin{center}
		\includegraphics[width=0.9\linewidth]{Images/DataTransformation.png}
		\caption{Result after applying data transformation}
		\label{DataTrans} 
	\end{center}
\end{figure}

\section{Data Mining}
Data mining involves extracting hidden patterns and anomalies from datasets. In this project, multiple approaches were utilized, combining feature extraction, clustering and anormal detection for potential threat analysis.
%	clustering and anomaly detection were utilized to uncover system irregularities and potential security threats efficiently.

\subsection{Feature Extraction}
Feature extraction involves converting raw data into a structured format suitable for machine learning.
\begin{itemize}
	\item \textbf{Path-Based Features (Approach 1 \& 2)}
	\begin{itemize}
		\item Extracted features based on \SHELL{Path} attribute such as Path length, Presence of special characters, SQL keywords, Path traversal attempts and Suspicious file extensions.
		\item Applied TF-IDF vectorization on the \SHELL{Path} attribute to represent API endpoint access as numerical features.
	\end{itemize}
	\item \textbf{User-Agent Analysis (Approach 3)}
	\begin{itemize}
		\item Analyzed \SHELL{User-Agent} strings to identify patterns associated with malicious or unusual behaviors.
	\end{itemize}
\end{itemize}

\subsection{Clustering and Anomaly Detection}
Three complementary approaches were used to detect anomalies:
\subsubsection{Approach 1}
\begin{itemize}
	\item This approach combined extracted path features (e.g., path length, special characters) with frequency metrics and numeric attributes like HTTP status codes.
	\item Anomalies were detected using DBSCAN for clustering and Isolation Forest for outlier detection.
\end{itemize}
\subsubsection{Approach 2}
\begin{itemize}
	\item Used TF-IDF vectorized features and numeric attributes for clustering with DBSCAN.
	\item Combined DBSCAN with Isolation Forest for robust anomaly detection, flagging requests that deviated from expected patterns.
\end{itemize}
\subsubsection{Approach 3}
\begin{itemize}
	\item Focused on analyzing User-Agent patterns to identify suspicious behaviors, integrating these insights with machine learning models for effective anomaly detection.
\end{itemize}

\section{Model}
The model selection was driven by the need to perform both clustering and anomaly detection effectively. A combination of unsupervised and ensemble-based learning techniques was employed to analyze request patterns and identify anomalies.

\subsection{DBSCAN}
DBSCAN is a density-based clustering algorithm, groups data points that are closely packed together based on a density threshold. Data points far from any dense cluster are treated as "noise" or anomalies.


\subsubsection{Key Parameters}
\begin{itemize}
	\item \textbf{eps}: Set to 0.5, defining the maximum distance between points to form a cluster.
	\item \textbf{min\_samples}: Set to 5, specifying the minimum number of points required to form a cluster.
\end{itemize}
\subsubsection{Outcome}
Requests not assigned to any cluster (cluster = -1) were flagged as anomalies.

\subsection{Isolation Forest}
Isolation Forest is an ensemble learning technique designed to detect outliers by isolating data points. It works on the principle that anomalies are easier to isolate because they have unique features.
\subsubsection{Key Parameters}
\begin{itemize}
	\item \textbf{n\_estimators:} Set to 100, specifying the number of decision trees.
	\item \textbf{contamination:} Set to 0.01, indicating the proportion of expected anomalies in the dataset.
	\item \textbf{random\_state:} Ensured reproducibility of results with a fixed seed value (42).
\end{itemize}
\subsubsection{Outcome}
Requests identified as anomalies were flagged with a value of -1.

\section{Validation/Verification}
Validation and verification are critical steps to ensure the reliability and accuracy of the data mining and anomaly detection process. This section outlines the methods used to validate the models and verify the results.

\subsection{DBSCAN Validation}
The results of the DBSCAN clustering were validated as follows:
\begin{itemize}
	\item The clustering results were reviewed to ensure that data points in the same cluster exhibited similar patterns.
	\item Anomalies (\texttt{Cluster = -1}) were manually inspected to confirm that they represented requests deviating from normal patterns.
\end{itemize}

\subsection{Isolation Forest Validation}
The Isolation Forest model's performance was validated using the following methods:
\begin{itemize}
	\item \textbf{Anomaly Score Distribution}: The distribution of anomaly scores was evaluated to differentiate anomalies from regular requests.
	\item \textbf{Manual Inspection}: A subset of flagged anomalies was reviewed to confirm their unusual characteristics.
\end{itemize}

\section{Data Visualization}

\begin{figure}
	\begin{center}
		\includegraphics[width=0.7\linewidth]{Images/PowerBI1.png}
		\caption{Bar chart illustrating the distribution of anomalies based on Path, TraceId, and HTTP Status Code}
		\label{PowerBI1} 
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[width=0.7\linewidth]{Images/PowerBI2.png}
		\caption{Bar chart visualizing anomalies identified using Path and TraceId, based on clustering efficiency and deviations from normal access patterns}
		\label{PowerBI2} 
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[width=0.7\linewidth]{Images/PowerBI3.png}
		\caption{Analysis of anomalies detected based on User-Agent behaviors}
		\label{PowerBI3} 
	\end{center}
\end{figure}

The final step in the KDD process is representing the acquired knowledge in an understandable form and using it to make decisions. To effectively demonstrate and view the output of our data mining techniques we used Power Bi tool where we created a dashboard for approach 1, approach 2 and approach 3 as shown in the figure ~\ref{PowerBI1}, ~\ref{PowerBI2} and ~\ref{PowerBI3} respectively, for visualizing the Paths and the User-agents which we identified as suspicious threat. Power BI's dashboards and reports are excellent for this purpose, as they can effectively communicate insights to all levels of an organization.

\section{Conclusion}
This project successfully utilized data mining and anomaly detection techniques to uncover irregularities within system log files. By utilizing advanced machine learning algorithms such as DBSCAN and Isolation Forest, we were able to cluster request patterns and detect anomalies with precision, resulting in a reliable and efficient detection process. The implementation followed the Knowledge Discovery in Databases (KDD) process, ensuring a systematic approach to data selection, preparation,  transformation and analysis, which enhanced the accuracy and relevance of the findings.
