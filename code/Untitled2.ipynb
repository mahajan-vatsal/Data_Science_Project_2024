{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows processed: 190000\n",
      "Total anomalies detected: 9304\n",
      "\n",
      "Sample of detected anomalies:\n",
      "                                                Path  HTTP Status Code  \\\n",
      "0                                       /api/healthz             503.0   \n",
      "1  /api/v1/references/HTTPS:%2F%2FOWDS.ORG%2FAF.p...             200.0   \n",
      "2  /api/v1/documents/33c6ab11-ed75-46b1-aeb9-58f2...               0.0   \n",
      "3                                              /owa/             404.0   \n",
      "4                                       /api/healthz             503.0   \n",
      "\n",
      "                                          User Agent  \n",
      "0                                         curl/8.5.0  \n",
      "1  Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...  \n",
      "2  Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; ...  \n",
      "3                                        curl/7.54.0  \n",
      "4                                         curl/8.5.0  \n",
      "\n",
      "HTTP Status Code distribution in anomalies:\n",
      "HTTP Status Code\n",
      " 404.0    3000\n",
      " 201.0    2824\n",
      " 503.0    1784\n",
      " 204.0     609\n",
      " 200.0     506\n",
      " 401.0     402\n",
      " 0.0        48\n",
      "-1.0        35\n",
      " 500.0      34\n",
      " 400.0      28\n",
      " 202.0      13\n",
      " 409.0       8\n",
      " 415.0       7\n",
      " 301.0       3\n",
      " 499.0       2\n",
      " 403.0       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "import re\n",
    "\n",
    "# File path\n",
    "file_path = '/Users/vatsal/Desktop/consolidated_trace_data_final2.csv'  # Adjust as needed\n",
    "\n",
    "# Function to extract features from the 'Path' column\n",
    "def extract_path_features(path):\n",
    "    features = {\n",
    "        'path_length': len(str(path)),\n",
    "        'special_chars': len(re.findall(r'[^a-zA-Z0-9/]', str(path))),\n",
    "        'has_sql_keywords': 1 if re.search(r'(select|union|delete|drop|insert|exec|update)', str(path).lower()) else 0,\n",
    "        'has_path_traversal': 1 if '..' in str(path) or '//' in str(path) else 0,\n",
    "        'has_suspicious_extensions': 1 if re.search(r'\\.(php|asp|aspx|exe|bat|cmd)$', str(path).lower()) else 0\n",
    "    }\n",
    "    return pd.Series(features)\n",
    "\n",
    "# Initialize results storage for anomalies\n",
    "chunk_results = []\n",
    "\n",
    "# Read and process the dataset in chunks\n",
    "chunk_size = 10000\n",
    "chunks = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Fill missing values\n",
    "    chunk['HTTP Status Code'].fillna(-1, inplace=True)\n",
    "    chunk['User Agent'].fillna('Unknown', inplace=True)\n",
    "\n",
    "    # Extract path features\n",
    "    path_features = chunk['Path'].apply(extract_path_features)\n",
    "    \n",
    "    # Add frequency of paths as a feature\n",
    "    chunk['path_frequency'] = chunk['Path'].map(chunk['Path'].value_counts())\n",
    "    \n",
    "    # Combine features\n",
    "    X = pd.concat([\n",
    "        path_features,\n",
    "        pd.get_dummies(chunk['HTTP Status Code'].astype(int), prefix='status'),\n",
    "        chunk[['path_frequency']]\n",
    "    ], axis=1)\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Anomaly Detection: Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "    chunk['is_anomaly_iso'] = iso_forest.fit_predict(X_scaled)\n",
    "\n",
    "    # Anomaly Detection: DBSCAN\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    chunk['is_anomaly_dbscan'] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "    # Combine results from both models\n",
    "    chunk['is_anomaly_combined'] = (chunk['is_anomaly_iso'] == -1) | (chunk['is_anomaly_dbscan'] == -1)\n",
    "\n",
    "    # Store anomalies from this chunk\n",
    "    chunk_anomalies = chunk[chunk['is_anomaly_combined']]\n",
    "    chunk_results.append(chunk_anomalies)\n",
    "\n",
    "# Combine anomalies from all chunks\n",
    "anomalies = pd.concat(chunk_results, ignore_index=True)\n",
    "\n",
    "# Save anomalies to a CSV file\n",
    "output_file = \"anomalies_detected2.csv\"\n",
    "anomalies.to_csv(output_file, index=False)\n",
    "\n",
    "# Display summary\n",
    "print(f\"Total number of rows processed: {chunk_size * len(chunk_results)}\")\n",
    "print(f\"Total anomalies detected: {len(anomalies)}\")\n",
    "print(\"\\nSample of detected anomalies:\")\n",
    "print(anomalies[['Path', 'HTTP Status Code', 'User Agent']].head())\n",
    "\n",
    "# Analyze HTTP Status Code distribution in anomalies\n",
    "print(\"\\nHTTP Status Code distribution in anomalies:\")\n",
    "print(anomalies['HTTP Status Code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
